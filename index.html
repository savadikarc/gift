<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="GIFT: Generative Interpretable Fine-Tuning">
  <meta name="keywords" content="PEFT">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>GIFT: Generative Interpretable Fine-Tuning Transformers</title>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <!-- <link rel="icon" href="./static/images/favicon.svg"> -->

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop decorate-purple">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title"><span class="dnerf">Gift</span>: Generative Interpretable Fine-Tuning</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://savadikarc.github.io">Chinmay Savadikar</a><sup>1</sup>,</span>
            <span class="author-block">
              Xi Song<sup>2</sup>,</span>
            <span class="author-block">
              <a href="https://ece.ncsu.edu/people/twu19/">Tianfu Wu</a><sup>1</sup>,
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>North Carolina State University,</span>
            <span class="author-block"><sup>2</sup>An Independent Researcher</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://arxiv.org/pdf/2312.00700.pdf"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://arxiv.org/abs/2312.00700"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>ArXiv</span>
                </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/savadikarc/gift"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <img src="./static/images/gift.svg"
                 class="interpolation-image"
                 alt="GIFT."/>
      <h2 class="subtitle has-text-justified">
        LoRA (a) is a layer-specific fine-tuning paradigm by  learning the low-rank weight residuals directly as model 
        parameters in optimization. Our GIFT (b) is a deep weight-residual learning paradigm, which is shared across all 
        the layers by treating pretrained weights from different fine-tuning layers (e.g., \(L\)) as a batch of input 
        ''data'' and directly computes fine-tuned weights via residual learning. We show that simply 
        parameterizing GIFT with two plain linear layers (without bias terms) is surprisingly effective, i.e., 
        \(\hat{\omega}=\omega \cdot (\mathbb{I}+\phi_{d_{in}\times r}\cdot \psi_{r\times d_{in}})\) where \(\mathbb{I}\) 
        is an identity matrix. \(\Theta=(\phi, \psi)\) are the learnable parameters of the two linear projections (layers) of GIFT 
        with \(r\) being a hyper-parameter. During fine-tuning, the loss function for a downstream task is optimized with respect 
        to the parameters of GIFT \((\Theta)\). After applying GIFT to a linear layer in the pretrained Transformer, the output of the layer can be written as <span id="gift-out">\(\hat{y}_{ N\times d_{out}} = x_{ N\times d_{in}} \cdot \underbrace{\hat{\omega}^{\top}}_{\text{GIFTed weights}} + b =\underbrace{x_{ N\times d_{in}} \cdot (\mathbb{I} +  \psi^{\top}\cdot \phi^{\top})}_{\text{GIFTed activation, denoted by }  \hat{x}_{ N\times d_{in}}} \cdot \omega^{\top} + b\)</span>.
        With this formulation, our GIFT can be equivalentlty applied to the activation/representation space.
      </h2>
    </div>
  </div>
</section>

<section class="hero is-light">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <h2 class="title is-3">How expressive is this simple GIFT?</h2>
      <img src="./static/images/teaser.svg"
                 class="interpolation-image"
                 alt="GIFT."/>
      <h2 class="subtitle has-text-justified">
        <i>Left</i>: Comparison of accuracy vs. parameters between different PEFT methods and GIFT on the Commonsense170k benchmark
        using LLaMa-1 (7B), and Llama-2 (7B)-3 (8B) models. GIFT obtains better performance with significant;y fewer parameters.
        <i>Right</i>: When GIFT is applied to fine-tune the projection layers in the multi-head self-attention modules of Vision Tranformers on image classification tasks,
        the output of the first linear layer <span id="cluster-eq">\((C_{d_{out}\times r}=\omega_{d_{out}\times d_{in}}\cdot \phi_{d_{in}\times r})\)</span> plays the role 
        of a \(r\)-way segmentation/token-clustering head. This localization ability emerges as a by-product without any direct supervision for the segmentation maps,
        using the standard cross-entropy loss during fine-tuning.
        The maps can form on objects/parts in images, even handling occlusions (e.g., the bird body in the right-bottom row), 
        and finding relevant objects (full bird, head in right-top row) even if the object occupies a small part of the image.
      </h2>
    </div>
  </div>
</section>

<section class="hero" id="experiments">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <h2 class="title is-3">Results: Commonsense170k</h2>
      <img src="./static/images/results-commonsense.png"
                class="interpolation-image"
                alt="GIFT."/>
    </div>
  </div>
</section>

<section class="hero is-light" id="more-interpretability">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <h2 class="title is-3">Results: Visual Recognition</h2>
      <h2 class="subtitle has-text-justified" style="margin-bottom: 2em;">
      For the visual recognition tasks, we fine-tune the ViT-B/16 model pretrained on ImageNet21k using a supervised objective (which is available in <a href="https://github.com/huggingface/pytorch-image-models" style="font-family: Courier">timm</a>).
      </h2>
      <h2 class="title is-4">FGVC Benchmark</h2>
      <img src="./static/images/results-fgvc.png"
                 class="interpolation-image"
                 alt="GIFT."/>
      <h2 class="subtitle has-text-justified">
        GIFT performs better than all the baselines on the FGVC benchmark. Moreover, we show that meaningful visual segmentation/token-clustering maps are formed. 
        We show examples of head, wings and legs of birds in the <i>top-left</i>,  examples of flower 
        petals in the <i>top-right</i>, examples of head, ears and legs of dogs in the <i>bottom-left</i>, and 
        examples of tires, windshield and bumper of cars in the <i>bottom-right</i>. We can see global (object level) 
        as well as part-level maps.
      </h2>
      <img src="./static/images/interpretability-all.svg"
                 class="interpolation-image"
                 alt="GIFT."/>

      <h2 class="title is-4" style="margin-top: 1rem">VTAB Benchmark</h2>
      <img src="./static/images/results-vtab.png"
                class="interpolation-image"
                alt="GIFT."/>
    </div>
  </div>
</section>

<section class="hero" id="experiments">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <h2 class="title is-3">Results: Natural Language Understanding (GLUE)</h2>
      <img src="./static/images/results-glue.png"
                class="interpolation-image"
                alt="GIFT."/>
    </div>
  </div>
</section>

<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>
@misc{savadikar2024gift,
  title={GIFT: Generative Interpretable Fine-Tuning}, 
  author={Chinmay Savadikar and Xi Song and Tianfu Wu},
  year={2024},
  eprint={2312.00700},
  archivePrefix={arXiv},
  primaryClass={cs.CV}
}
    </code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link"
         href="https://arxiv.org/pdf/2312.00700.pdf">
        <i class="fas fa-file-pdf"></i>
      </a>
      <a class="icon-link" href="https://github.com/savadikarc" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div>
    <div class="columns is-centered">
        <a href="https://research.ece.ncsu.edu/ivmcl/" class="external-link">
           <u>About iVMCL</u>
        </a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is adapted from <a href="https://nerfies.github.io/">Nerfies</a> and modifed by <a href="https://savadikarc.github.io">Chinmay Savadikar</a>.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
